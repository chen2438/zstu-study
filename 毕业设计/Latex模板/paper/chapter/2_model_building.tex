\section{地库车位设计方式及深度强化学习理论}
\subsection{地库设计理论}
\subsubsection{地库设计要点}
地库设计的要点包括但不限于以下几个方面：
\begin{enumerate}
    \item 车位数量规划：地库设计需充分考虑实际需求确定车位数量，尽可能多地提供车位，以确保最佳的车位利用率和通行便利性。
    \item 车位排列方式：地库设计可采用多种排列方式，包括横排、${30}^\circ$、${45}^\circ$、${60}^\circ$和竖排等，以满足不同场地和需求条件下的停车需求。
    \item 车位尺寸及道路设计： 地下停车场的车位尺寸和道路设计应统一规格，符合国家标准和规范要求，确保车辆安全通行和便捷停放，以实现最佳的停车效率、通行安全性和舒适性。
    \item 出入口设置：地库的出入口应设置合理，便于车辆的进出和行人的通行，同时要考虑交通流量和安全性。
    \item 承重柱布局：地下停车场的承重柱应科学布局，以支撑结构并确保地下停车场的安全性。
    \item 安全设施配置：地下停车场的设计应考虑包括消防通道、安全出口和灭火器等设施的配置，以应对紧急情况，确保人员安全疏散和火灾等突发事件的应对能力。
\end{enumerate}
\subsubsection{柱网设置}
在地库设计中，柱网的设置是至关重要的一环，它直接关系到停车场的结构稳定性、通行便利性以及车辆与构筑物之间的净距。柱网是由多个承重柱组成的支撑结构，用于支撑停车场的天花板和地面，并确保停车场的安全运行。
\begin{enumerate}
    \item 柱距间净距要求： 车辆与车辆之间、车辆与构筑物之间的净距应符合相关规定，以确保车辆的安全通行和空间利用。根据规定，地下车库柱距间停放1辆、2辆或3辆汽车时所需的最小柱距分别为3.1米、5.6米和8.1米。
    \item 承重柱布局： 承重柱的布局应合理规划，均匀分布于停车场各个区域，并具备足够的强度和稳定性以支撑停车场结构。
    \item 施工和验收： 施工过程中需严格按设计要求和规范进行操作，确保承重柱的布局、尺寸和材料符合标准。完成后需进行质量检查和验收，确保柱网的稳定性和安全性符合设计要求。
\end{enumerate}

考虑到具体的方案落地情况，柱网间距、车位面积以及通道间隔之间存在着密切的关系，彼此相互制约，直接影响着最终的车位数量和停车场的整体效果。
\subsubsection{车位排列方式}
在地库排布中，为了提高设计的灵活性和高效性，本文引入了车位模块\cite{1020726891.nh}这一概念。车位模块是指一个矩形区域，内部仅能容纳同排列类型车位，并根据一定规律进行摆放。其中主要包含水平、30°、45°、60°和垂直等，如图~\ref{fig:car_paving}。车位模块的引入可以简化地库设计的复杂度，提高设计的灵活性和可扩展性，同时也有利于后续的优化和调整。
\begin{figure}[!htb]
	\centering  
	\subfigure[${45}^\circ$排布]{
		\includesvg[width=0.365\linewidth]{./2/排列45}}
	\subfigure[${30}^\circ$排布]{
		\includesvg[width=0.555\linewidth]{./2/排列30}}
	  \\
	\subfigure[竖直排布]{
		\includesvg[width=0.266\linewidth]{./2/排列垂直}}
	%\quad
	\subfigure[${60}^\circ$排布]{
		\includesvg[width=0.301\linewidth]{./2/排列60}}
	\subfigure[水平排布]{
		\includesvg[width=0.313\linewidth]{./2/排列横向}}
	\caption{车位排列方式}
    \label{fig:car_paving}
\end{figure}

根据图示计算当均容纳六车位时，各类型车位模块长宽：
\begin{equation}
    sqaure_{90^\circ} = (3625,250) 
\end{equation}
\begin{equation}
    sqaure_{60^\circ} = (300 + \frac{3475}{3}\sqrt{3},300\sqrt{3}+125) 
\end{equation}
\begin{equation}
    sqaure_{45^\circ} = (1700\sqrt{2}, 425\sqrt{2})
\end{equation}
\begin{equation}
    sqaure_{30^\circ} = (300\sqrt{3}+3175,300+125\sqrt{3}) 
\end{equation}
\begin{equation}
    sqaure_{0^\circ} = (3725,250)
\end{equation}
其模块总面积大小排序为：$sqaure_{90^\circ}<sqaure_{0^\circ}<sqaure_{45^\circ}<sqaure_{60^\circ}<sqaure_{30^\circ}$。

在本项目中，选择角度为0°和90°的车位模块进行排放，主要有以下原因：

\begin{enumerate}
\item 灵活性和空间利用率：0°和90°的车位模块可以根据地库的形状、尺寸、出入口位置、承重柱位置等因素进行调整，以实现最佳的空间利用率和停车容量。
\item 安全性和便捷性：这两种排放方式使得车位和车道之间排列紧凑、整齐，便于承重柱的设置，确保车辆安全通行和停放，便于进出。
\end{enumerate}

在0°和90°之间，我们优先选择90°的车位模块进行排放，原因如下：

\begin{enumerate}
\item 更高的空间利用率和停车容量：90°车位模块在同样的面积内能够提供更多的停车位。
\item 进出便利性：90°车位模块便于驾驶员操作，提高停车效率。
\item 适应性：90°车位模块更好地配合承重柱布局，减少难以利用的空间。
\end{enumerate}

\subsection{深度强化学习理论}
\subsubsection{强化学习}
强化学习是一种机器学习方法，通过智能体和环境间的不断交互，学习最佳的行为策略。\cite{sutton2018reinforcement}与监督学习和无监督学习不同，强化学习中的反馈是交互产生的，而并非使用预先标记的数据。这种交互过程可以形式化地描述为马尔可夫决策过程（MDP），其中主要包含智能体、环境、状态、行为、策略、奖励等。
\begin{figure}[!htb]
    \centering
    \includesvg[width=0.7\textwidth]{2/强化学习}
    \caption{强化学习示意图}
    \label{fig:rl_structure}
\end{figure}

在图~\ref{fig:rl_structure} 所示的强化学习示意图中，智能体与环境交互，智能体根据环境状态和奖励更新策略，以最大化累积奖励。

通常，强化学习可以被分为两大类别。其中，基于值函数的强化学习侧重于学习状态或状态-行为对的价值，以间接确定最优策略；而基于策略的强化学习则直接学习在给定状态下采取的行为，以最大化累积奖励。
\subsubsection{马尔可夫决策过程与贝尔曼方程}
马尔可夫决策过程（MDP）描述了强化学习问题的数学框架，它的基础是具有“无记忆性”且随机的马尔可夫过程，且相较于马尔可夫奖励过程，进一步引入了行为这一概念。在 MDP 中，智能体在每个时间步 $t$ 处于某个状态 $s_t$，然后选择行为 $a_t$，环境根据状态和行为的组合给予奖励 $r_{t+1}$，智能体根据奖励和下一个状态 $s_{t+1}$ 更新策略。MDP 可以表示为一个五元组 $(S, A, P, R, \gamma)$，表示为$$P(S_{t+1}=s', R_{t+1}=r|S_t=s, A_t=a) = P_{sa}(s', r).$$

贝尔曼方程（Bellman Equation）是MDP的核心，它描述了状态函数（$V$）或行为函数（$Q$）之间的递归关系，通过将两者相关联，向智能体提供学习和决策的基础。

对于状态值函数 $V$，贝尔曼方程可以表示为：
\begin{equation}
    V_\pi(s) = \sum_{a \in A} \pi(a|s) ( r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) V^\pi(s') )
\end{equation}

对于行为值函数 $Q$，贝尔曼方程可以表示为：
\begin{equation}
    Q_\pi(s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) \sum_{a' \in A} \pi(a'|s') Q^\pi(s',a')
\end{equation}

这些方程使智能体能够通过迭代更新值函数来优化其策略，以最大化累积奖励。

\subsubsection{深度强化学习}
深度强化学习是一种结合了深度学习和强化学习的方法\cite{mnih2013playing}，旨在解决复杂环境和大型状态空间下的决策问题。\cite{1023788635.nh}其核心思想是利用深度神经网络来近似值函数或策略函数，以应对大型状态空间和连续行为空间的挑战。

在深度强化学习中，典型的基于值函数的方法包括基于深度学习的Deep Q-Network（DQN）和Double DQN，这些算法已成功应用于诸如Atari游戏和机器人控制等领域。而基于策略的方法则包括策略梯度方法，例如确定性策略梯度（DDPG）和TRPO（Trust Region Policy Optimization），这些方法在连续行为空间和高维状态空间的问题上表现出色。

根据强化学习的两大类型，本文选择了其中两个较为主流的算法进行介绍。
\paragraph{DQN算法}
DQN（Deep Q-Network）是将神经网略（neural network）和Q-learning结合，利用深度神经网络来近似值函数（Q值函数）\cite{mnih2013playing}，从而实现对复杂环境的学习和决策。其核心思想为以下三个部分：
\begin{itemize}
    \item 经验回放：引入经验回放机制，将智能体与环境交互得到的经验存储在经验回放缓冲区中，并通过随机抽样的方式将这些经验用于训练深度神经网络。这一机制的作用在于减少样本之间的相关性，从而提高训练的稳定性和收敛速度。同时，这也使得DQN算法可以离线（off-line）地从存储的经验中学习，而不需要依赖实时的环境反馈。
    \item 固定目标网络是DQN算法的关键部分之一。它使用两个神经网络来估计状态行为值函数：一个用于选择行为（行为网络），另一个用于评估行为的价值（目标网络）。在训练过程中，目标网络的参数会定期固定一段时间，然后周期性地更新为行为网络的参数。这个步骤的目的是为了稳定训练过程，防止参数的不稳定性影响算法的收敛性。
    \item $\epsilon$-贪心策略：以$\epsilon$的概率随机选择行为，反之选择当前状态下具有最大Q值的行为。这一策略的作用在于在探索和利用之间取得平衡，从而更好地探索环境并学习最优策略。
\end{itemize}

下面是DQN算法的基本流程，同时结合图~\ref{fig:dqn} 进行了图像化展示：
\begin{enumerate}
    \item 初始化深度神经网络的参数。
    \item 与环境交互，根据当前策略选择行为，并观察接收反馈。
    \item 将$(s, a, r, s')$存储在经验回放缓冲区中。
    \item 从经验回放缓冲区中随机抽样一批数据，用于训练深度神经网络。
    \item 更新行为网络的参数，使其逼近Q值函数。
    \item 更新目标网络的参数，使其逼近行为网络的参数。
    \item 重复步骤2~6，直到满足停止条件。
\end{enumerate}
\begin{figure}[!htb]
    \centering
    \includesvg[width=0.7\textwidth]{2/dqn}
    \caption{DQN算法流程图}
    \label{fig:dqn}
\end{figure}

\paragraph{PPO算法}
PPO（Proximal Policy Optimization）是一种基于策略优化的强化学习算法\cite{schulman2017proximal}。它通过在更新策略时引入一个剪切项，保证每次更新的策略变化在一个可控的范围内，从而提高学习的稳定性和收敛速度。其核心思想为以下四个部分：
\begin{itemize}
    \item 近端优化：PPO通过限制每次策略更新的幅度，确保策略变化在一个合理的范围内，从而防止过度更新引发的不稳定性和收敛困难。
    \item 剪切项：PPO引入了一个剪切项，用于限制更新前后策略的KL散度（Kullback-Leibler Divergence），保证更新的策略变化不会过大。
    \item 多步优化：PPO在更新策略时通常使用多步优化，即利用多个时序上连续的样本来计算近似优势函数，提高优化的效率和稳定性。
    \item 并行化：PPO算法具有出色的并行化性能，可以有效地利用多核CPU或分布式计算资源，加速算法的收敛速度和训练效率。
\end{itemize}

下面是PPO算法的基本流程，同时结合图~\ref{fig:ppo} 进行了图像化展示：
\begin{enumerate}
    \item 初始化策略函数的参数。
    \item 与环境交互，根据当前策略选择行为，并观察环境的反馈。
    \item 计算策略函数的梯度，并根据剪切项和近端优化的原则更新策略参数。
    \item 重复步骤2-3，直到满足停止条件。
\end{enumerate}
\begin{figure}[!htb]
    \centering
    \includesvg[width=\textwidth]{2/ppo}
    \caption{PPO算法流程图}
    \label{fig:ppo}
\end{figure}

\subsection{决斗网络}
决斗网络（Dueling Network）是一种深度强化学习架构，它通过分离值函数和优势函数的学习来实现，从而提高学习的效率和稳定性，如图~\ref{fig:duiling_net} 所示。
\begin{figure}[!htb]
    \centering
    \includesvg[width=0.7\linewidth]{2/duiling_net}
    \caption{\label{fig:duiling_net}决斗网络示意图}
\end{figure}

传统的Q-learning直接学习一个行为值函数$Q(s, a)$，但这种方法可能会忽视了一个重要的事实，即在很多情况下，无论采取什么行为，状态的价值都是相同的。因此，行为值函数可进一步分解为状态值函数$V(s)$和优势函数$A(s, a)$。状态值函数表示当前状态的价值，而优势函数用于与其他行为进行比较，计算与其他行为相比的价值。

决斗网络的架构包含两个独立的神经网络，一个用于学习状态值函数，另一个用于学习优势函数。这两个网络共享底层的特征提取层，但有各自的输出层。最后，通过一个特殊的聚合层将这两个网络的输出合并，以得到行为值函数。

决斗网络的优点是，它可以更准确地学习状态值函数和优势函数，从而提高学习的效率和稳定性。此外，由于它可以分别学习状态值函数和优势函数，因此它可以更好地处理那些状态值变化大但优势函数变化小的情况，从而提高智能体的性能。

\subsection{优先级经验回放}
优先级经验回放（Prioritized Experience Replay，简称 PER）改进了传统的经验回放机制。在传统的经验回放中，智能体从其经验池（也称为回放缓冲区）中均匀随机地抽取过去的经验进行学习。然而，这种方法忽略了一些经验可能比其他经验更重要的事实。而优先级经验回放通过为每个经验分配优先级，并根据这些优先级来抽取经验，从而解决了这个问题。

\paragraph{\textbf{SumTree}}
在PER中，SumTree常被用于存储和抽取经验。SumTree是一种特殊的二叉树，其每个节点的值等于其子节点的值之和。叶节点存储经验的优先级（通常为TD误差），非叶节点存储其子节点优先级之和。如图~\ref{fig:sumtree} 所示：
\begin{figure}[!htb]
    \centering
    \includesvg[width=0.5\linewidth]{2/sumtree}
    \caption{\label{fig:sumtree}SumTree示意图}
\end{figure}

经验的抽取过程基于其优先级，优先级总和被分为等量的区间，每个区间内随机选取一个数，然后在SumTree中找到对应的经验。例如，假设抽取3个经验，并按照优先级总和（在此例中为49）分为3个区间：$[0, 16]$, $[16, 32]$, $[32, 49]$。在每个区间内随机选取一个数，然后在SumTree中找到对应的经验，若大于左节点，则向右，并将当前数值减去左节点数值，否则向左，直到找到一个叶节点。

通过这种方式，可在每个区间内进行均匀采样，最后得到3个经验。这样，优先级高的经验就有更高的概率被抽取到。

\paragraph{优先级的更新}
在PER中，经验的优先级是动态更新的，其依据是智能体的学习过程中计算的TD误差。TD误差，即智能体预测的Q值与实际Q值之间的差距，可以用以下公式表示：

\begin{equation}
    TD_{error} = r + \gamma \cdot \max_{a'}Q(s', a') - Q(s, a)
\end{equation}

经验的优先级根据TD误差的大小进行调整，TD误差大的经验优先级提高，反之则降低。在SumTree上更新时，需先找到这个经验对应的叶节点并更新，再递归更新所有包含该叶节点的非叶节点的值。优先级的更新公式如下：

\begin{equation}
    P(i) = |TD_{error}| + \epsilon
\end{equation}

其中，$P(i)$是第$i$个经验的优先级，$|TD_{error}|$是TD误差的绝对值，$\epsilon$是一个很小的正数，用来确保优先级永远不会是0。

通过这种方式可以确保SumTree始终反映了所有经验的最新优先级，从而使得优先级高的经验有更高的概率被抽取到。

\paragraph{重要性采样权重}
在优先级经验回放（PER）中，重要性采样权重（ISW）起到关键作用，它不仅影响经验的选择，也调整学习过程。ISW为每个经验计算一个权重，用于调整学习目标。

ISW的计算公式如下：

\begin{equation}
    ISW(i) = \left( N \cdot P(i) \right)^{-\beta}
\end{equation}

其中，$N$是经验的总数，$P(i)$是第$i$个经验的优先级，$\beta$是一个介于0和1之间的参数，用来控制重要性采样权重的影响程度。

ISW的主要作用是平衡各经验在学习过程中的影响，避免优先级高的经验被过度采样，优先级低的经验被忽视。通过ISW，所有经验都有被学习的机会，从而使得智能体可以从所有的经验中学习。

在实际应用中，每个更新步骤开始时，先计算所有经验的ISW，然后在计算学习目标时使用这些权重，以确保每个更新步骤都考虑到所有经验的优先级。

\subsection{工具选型}
\subsubsection{Pytorch}
PyTorch是一款基于Python的库，以其在科学研究、教育和工业界广泛应用的深度学习框架而闻名。它提供了基于数组的编程模型，通过GPU加速实现微分功能，并与Python生态系统中的自动微分紧密集成。同时，PyTorch自然地与标准绘图、调试和数据处理工具集成，采用命令式编程模型。支持与外部库双向交换数据，用户可以根据项目需求或性能要求自由更换组件。PyTorch实现了张量数据结构、GPU和CPU操作符以及基本的并行原语。它的自动微分系统包括对大多数内置函数的梯度计算，具有立即执行动态张量计算的能力，性能上与当前最快的深度学习库相媲美，在研究社区中备受欢迎（如2019年ICLR提交的296篇论文中提到的\cite{paszke2019pytorch}）。

在本项目中，本文选择使用Pytorch实现自定义的深度学习模型，并便捷地处理高维输入输出，同时利用分布式训练在多个GPU上并行训练，为项目实现提供了高效可靠的解决方案。
\subsubsection{pyautocad}
pyautocad\cite{shahzad2023implementing}是一款专注于与AutoCAD软件交互的Python库，通过AutoCAD的COM接口实现对AutoCAD对象模型的访问和操作。具有多版本支持、执行AutoLISP代码的功能，其直观且方便的API设计使得通过Python直接调用AutoCAD的对象和方法变得简便。尤其适用于需要自动化处理CAD图纸的项目，拥有丰富的社区支持和文档资源，为开发人员提供了可靠的工具。

在本项目中，pyautocad通过利用与AutoCAD的强大交互能力，高效地实现对CAD图纸的自动修改和处理，也便于生成后的进一步修改。pyautocad为项目提供了便捷而可靠的解决方案，使得与AutoCAD集成的开发变得更加顺畅。
\subsubsection{AutoCAD}
AutoCAD是由AutoDesk公司开发的计算机辅助设计（CAD）软件，广泛应用于建筑和土木工程等领域。它提供了强大的绘图和建模工具，可用于创建、编辑和分析二、三维设计。其用户友好的界面和丰富的功能集使得设计师能够高效地进行各种设计任务，从简单的图纸绘制到复杂的建筑模型。

在该项目中，AutoCAD提供了强大的绘图和建模工具，通过对于绘制在AutoCAD中图纸，使得设计师能够轻松编辑并更改生成后的图纸。其支持DWG格式的特性也确保了设计图的广泛可用性。此外，AutoCAD的灵活性和可扩展性使其能够与其他工具和编程语言进行集成，如通过pyautocad连接Python脚本，实现与算法代码的交互。这种整合为项目提供了便捷的工具，使得设计和修改车库布局更加直观、高效。AutoCAD在车库设计项目中的应用为设计师提供了一个全面的平台，满足各种设计需求，从而提高了设计效率和质量。